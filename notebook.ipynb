{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Preparing Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "\n",
    "def install_packages(*packages):\n",
    "    for package in packages:\n",
    "        # If package has been imported before, no need to install it\n",
    "        if package not in sys.modules:\n",
    "            !{sys.executable} -m pip install {package} --quiet\n",
    "            print(f\"Installed <{package}>\")\n",
    "\n",
    "install_packages(\n",
    "    'ast','nltk','bokeh','spacy','numpy','scipy','kneed','pandas','gensim',\n",
    "    'folium','plotly','sklearn','mlxtend','xgboost','shapely',\n",
    "    'seaborn','datetime','requests','lightgbm','nbformat',\n",
    "    'networkx','hyperopt','geopandas','matplotlib','fuzzywuzzy',\n",
    "    'mplcursors','collections','geodatasets'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import math\n",
    "import requests \n",
    "import nbformat\n",
    "import mplcursors \n",
    "import geodatasets\n",
    "import numpy as np\n",
    "import spacy as sp\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import networkx as nx\n",
    "import lightgbm as lgb\n",
    "import geopandas as gpd\n",
    "from fuzzywuzzy import fuzz\n",
    "import plotly.express as px\n",
    "from kneed import KneeLocator\n",
    "\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import shapely.geometry as geom\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, LeaveOneOut\n",
    "\n",
    "from bokeh.tile_providers import CARTODBPOSITRON\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining dataset *Chicago Crimes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_crimes = pd.read_csv('./data/Crimes___2017_to_Present.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_crimes.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data cleaning\n",
    "\n",
    "In this section we begin to analyze missing data. With this analysis, we've decided to first correct some incorrect data and then try to fill missing data with possible values. One example of this is the *District* feature. While we have missing data on said feature, we're first gonna correct some datapoints which contain incorrect values and only after do we fill the empty data with the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of null entries per feature\n",
    "missing_values = list(chicago_crimes.isna().sum())\n",
    "# missing values is a list of the number of missing values in each column\n",
    "\n",
    "cols = list(chicago_crimes.columns)\n",
    "col_final = []\n",
    "for i in range(len(cols)):\n",
    "    if (missing_values[i] == 0):\n",
    "        cols[i]=\"Others\"\n",
    "d = dict(zip(cols, missing_values)) # making a dicionary for the missing values\n",
    "\n",
    "print(\"Number of Missing Values per feature >>\")\n",
    "missing_vals = pd.DataFrame(d, index=[\"Missing Values\"]) # Making a custom dataframe from dict d\n",
    "missing_vals.head()\n",
    "\n",
    "# Plotting the missing values in the dataset\n",
    "x = list(d.keys())\n",
    "y = list(d.values())\n",
    "sns.barplot(x=x, y=y, palette=\"GnBu_d\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Missing Values in the Dataset\", fontdict = {'fontsize': 20})\n",
    "plt.ylabel(\"Count of missing values\", fontdict={'fontsize': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Variables\n",
    "cont = chicago_crimes._get_numeric_data().columns\n",
    "print(\"The continuous variables are: \",list(cont))\n",
    "# Categorical Variables\n",
    "print(\"The categorical variables are: \",list(set(chicago_crimes.columns) - set(cont)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting the districts\n",
    "\n",
    "Some of the datapoints have the incorrect *District* feature.<br>\n",
    "To correct this feature of some of the datapoints, we noticed that each *Beat* lies inside a specific *District*, as seen in the picture bellow.\n",
    "\n",
    "<img src=\"./data/notebook_imgs/Districts_VS_Beats.gif\" style=\"width:300px\">\n",
    "<figcaption style='font-size:10pt'>Fig.1 - <i>District</i> VS <i>Beat</i>. Data sources: <a href='https://data.cityofchicago.org/d/aerh-rz74'>Beat</a> | <a href='https://data.cityofchicago.org/d/fthy-xz3r'>District</a></figcaption>\n",
    "\n",
    "After noticing this, we decided to use the *Beat* feature to obtain the *District*.\n",
    "\n",
    "It must be also noted that since some *District* are incorrect, we need to find a way of obtaining the correct *District* each *Beat* is a part of.<br>\n",
    "The way we approached was by finding how many crimes happenned in each *Beat*/*District* combination and picking the correct *District* for each *Beat*. this is done by:<br>\n",
    "* Obtaining the number of **crimes** (count of *ID*) for each *Beat*/*District* combination\n",
    "* For each *Beat*, comparing the number of **crimes** in all *District*\n",
    "* We noticed that the in **all** *Beat* with multiple *District*, the *District* with the lowest number of crimes never exceeded 22, while the other *District* was never less than 1161\n",
    "* After noticing the above, we excluded all *Beat*/*District* with less than 23 crimes and this gave us our *Beat*/*District* combination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregating by \"Blocks\" and \"District\"\n",
    "beats = chicago_crimes.groupby(by=['Beat', 'District'], as_index=False)[['ID']].count()\n",
    "\n",
    "# Checking the beats with more than one district\n",
    "dups = beats[beats['Beat'].duplicated()]\n",
    "dups = beats[beats['Beat'].isin(dups['Beat'])]\n",
    "\n",
    "# For each of the beats with more than one district, check which is the maximum num of crimes of the district with less crimes\n",
    "# (this will make it so we can filter all beats/district with more crimes than this amount and get unique values\n",
    "# while also checking if the value makes it plausable to be error)\n",
    "# We will also keep track of the minimum of the maximum value, so we can check if the difference between the two is too big\n",
    "# If so, we can assume all the Beat/District with a num of crimes less than max_min_val to be an error\n",
    "max_min_val = 0\n",
    "min_max_val = dups['ID'].max()\n",
    "for beat in dups['Beat'].unique():\n",
    "    tmp = dups[dups['Beat']==beat]['ID']\n",
    "    max_min_val = max(tmp.min(), max_min_val)\n",
    "    min_max_val = min(tmp.max(), min_max_val)\n",
    "print(f'max_min_val VS min_max_val -> {max_min_val} | {min_max_val}') \n",
    "\n",
    "# Since the difference between the two values is really big, we'll ignore all rows with a number of crimes less than \"max_min_val\"\n",
    "# This way, we can get a single district for each Beat\n",
    "beats = beats[beats['ID'] > max_min_val]\n",
    "\n",
    "# Validating every beat has only a single district\n",
    "assert not beats['Beat'].duplicated().any()\n",
    "\n",
    "# Filtering only the columns needed in beats\n",
    "beats = beats[['Beat', 'District']]     # Each beat has a single District"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the association between Beat and District obtained above, we're gonna correct the districts of some rows\n",
    "# Join the correct district of each Beat to original dataframe\n",
    "chicago_crimes = chicago_crimes.merge(beats, how='inner', on=['Beat'], suffixes=('', '_new'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting coordinates\n",
    "\n",
    "Some crimes have coordinates too much far away from chicago, we're gonna pick those and correct them.\n",
    "\n",
    "The approach here will be to pick the average coordinate for the Block where the crimes happenned and updating the coordinates accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some points have really weird coordenates, we're gonna fix that by giving them the average coordinates of their \"Beat\"\n",
    "wrong_coords = chicago_crimes[chicago_crimes['Latitude']<=37]\n",
    "\n",
    "# Picking all the points with the same \"Beat\" as \"wrong_coords\" crimes\n",
    "beats = chicago_crimes[chicago_crimes['Beat'].isin(wrong_coords['Beat'].unique())]\n",
    "# Removing the \"wrong_coords\" crimes\n",
    "beats = beats[beats['Latitude'] > 37]\n",
    "# Aggregating by \"Beat\" and averaging coordinates\n",
    "beats = beats.groupby(by=['Beat'], as_index=False)[['Latitude', 'Longitude']].mean()\n",
    "# Correct the coordenates by giving the crimes the average coordinates of each Beat\n",
    "beats = beats.rename(columns={'Latitude': 'Lat', 'Longitude':'Lon'})\n",
    "chicago_crimes = chicago_crimes.merge(beats, on=['Beat'], how='left')\n",
    "\n",
    "# Correcting the coordinates of the Crimes\n",
    "chicago_crimes['Lat'] = chicago_crimes['Lat'].fillna(chicago_crimes['Latitude'])    # Columns from \"blocks\" which don't have latitude, must have correct values\n",
    "chicago_crimes['Lon'] = chicago_crimes['Lon'].fillna(chicago_crimes['Longitude'])   # Same as above for Longitude\n",
    "chicago_crimes = chicago_crimes.drop(columns = ['Latitude', 'Longitude']).rename(columns={'Lat': 'Latitude', 'Lon': 'Longitude'})   # Drop old Lat/Lon columns, and rename new ones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing some of the null values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latitude/Longitude\n",
    "\n",
    "The specific coordinates (lat/lon) may not give much information, so we've decided to fill the null values with data.<br>\n",
    "\n",
    "Since when one coordinate is null, the other also is, we're gonna apply the same logic for both of the features.<br>\n",
    "Using the *Beat* of each datapoint, we're gonna pick the average coordinates of said *Beat* and apply attribute that value to said datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average coordinates for each beat\n",
    "beats = chicago_crimes[['Beat', 'Latitude', 'Longitude']]   # Pick only required columns from original dataset\n",
    "beats = beats.dropna(how='any')                             # Delete any null datapoints to calculate averages\n",
    "beats = beats.groupby(by=['Beat'], as_index=False).mean()   # Agregate by beat, and average the coordinates \n",
    "\n",
    "# Add average Beat latitude/lon to original dataset as \"Latitude_new\" and \"Longitude_new\" \n",
    "chicago_crimes = chicago_crimes.merge(beats, how='left', on=['Beat'], suffixes=('', '_new'))\n",
    "\n",
    "# Create new lat/lon columns (by filling null values of latitude/longitude columns with the new values)\n",
    "chicago_crimes['Latitude_new']  = chicago_crimes['Latitude' ].fillna(chicago_crimes['Latitude_new' ])\n",
    "chicago_crimes['Longitude_new'] = chicago_crimes['Longitude'].fillna(chicago_crimes['Longitude_new'])\n",
    "\n",
    "\n",
    "# Validate that there are no NULL Latitude/Longitude\n",
    "assert not chicago_crimes['Latitude_new'].isnull().any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### District\n",
    "\n",
    "Since *Beat* is the smallest police geographical area, a *Beat* describes is more granular than any of *District*/*Ward*/*Community*.<br>\n",
    "We have 0 datapoints with a **NaN** *Beat*, so we can use that feature to obtain the missing values. <br>\n",
    "Whenever a datapoint doesn't have district, lookup other datapoints with the same *Beat* and we have our missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average coordinates for each beat\n",
    "beats = chicago_crimes[['Beat', 'District_new']]            # Pick only required columns from original dataset\n",
    "beats = beats.drop_duplicates()                             # Ignore duplicate values\n",
    "beats = beats.rename(columns={'District_new': 'District'})  # Renam District_new to District\n",
    "\n",
    "# Add default Beat District to original dataset as \"District_new\"\n",
    "chicago_crimes = chicago_crimes.merge(beats, how='left', on=['Beat'], suffixes=('', '_new2'))\n",
    "\n",
    "# Create new District columns (by filling null values of District columns with the new values)\n",
    "chicago_crimes['District_new'] = chicago_crimes['District_new'].fillna(chicago_crimes['District_new2'])\n",
    "chicago_crimes = chicago_crimes.drop(columns=['District_new2'])\n",
    "\n",
    "# Validate that there are no NULL District\n",
    "assert not chicago_crimes['District_new'].isnull().any()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting the Ward/Community Area\n",
    "\n",
    "As we corrected some of the coordinates and districts, some of *Ward*/*Community Area* are not the most up to date.<br>\n",
    "To correct this feature, we are going to obtain the data from the source directly and use spatial manipulation to obtain the correct values for each datapoint.<br>\n",
    "The sources were accessed through the Endpoint API available on the references bellow.\n",
    "\n",
    "The respective data sources are: <a href='https://data.cityofchicago.org/d/sp34-6z76'>Ward</a>, <a href='https://data.cityofchicago.org/d/cauq-8yn6'>Community Area</a>\n",
    "\n",
    "We used the following logic for the correction of the features mentioned:\n",
    "* Obtain a polygon describing each feature\n",
    "* Using spatial manipulation, use the coordinates of each datapoint to pick in which area they lie within\n",
    "* The \"left out\" datapoints*, pick the nearest region\n",
    "\n",
    "<div style=\"font-size:10pt\">*Datapoints whose coordinates do not lie within any region. This may be due to some rounding errors, which have a great impact on GPS coordinates, and some different projections</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining which features to correct, and where to fetch the data from as well as which column contains the \"content\" we want\n",
    "data_sources = {\n",
    "    'Ward': {\n",
    "        'url': 'https://data.cityofchicago.org/resource/k9yb-bpqx.json',    # Where to get the data from\n",
    "        'column_name': 'ward'                                               # Data we want to bring to our original dataset\n",
    "    },\n",
    "    'Community Area': {\n",
    "        'url': 'https://data.cityofchicago.org/resource/igwz-8jzy.json',    # Where to get the data from\n",
    "        'column_name': 'area_numbe'                                         # Data we want to bring to our original dataset\n",
    "    }\n",
    "}\n",
    "\n",
    "# Creating a GeoDataFrame object from original dataset (to be able to perform geography operations)\n",
    "df = gpd.GeoDataFrame(chicago_crimes, geometry=gpd.points_from_xy(chicago_crimes['Longitude_new'], chicago_crimes['Latitude_new']))\n",
    "\n",
    "\n",
    "# Iterating through all sources, since steps will be exactly the same\n",
    "for col in data_sources.keys():\n",
    "    # Fetching data from data_source (url obtained in DatasetDescription.docx document)\n",
    "    raw_data = requests.get(data_sources[col]['url'])\n",
    "    raw_data = raw_data.json()  # Obtain the JSON response from data_source\n",
    "\n",
    "    # Transform the geometry into a Shape object (easily read by GeoPandas)\n",
    "    for (i, region) in enumerate(raw_data):\n",
    "        # Replace data with new data\n",
    "        raw_data[i] = {\n",
    "            # Create a \"Geometry\" column which will be the geometry read by GeoPandas\n",
    "            'geometry': geom.shape(region['the_geom'])\n",
    "            # Create a new column which will store the \"ID\" of the region\n",
    "            ,col: region[data_sources[col]['column_name']]\n",
    "        }\n",
    "\n",
    "    # Converting the geometries to a GeoDataFrame (to be able to use geometry join)\n",
    "    regions = gpd.GeoDataFrame(raw_data)\n",
    "    # Renaming the \"region\" column to the same as \"col\"\n",
    "    regions = regions.rename(columns={regions.columns.difference(['geometry']).tolist()[0]: col})\n",
    "\n",
    "    # Merge data onto original (with geoDataFrame) dataset\n",
    "    df = df.sjoin(regions, how='left', predicate='within', lsuffix='old', rsuffix='new') # Will merge regions onto DataFrame by checking in which region each datapoint is contained\n",
    "    df = df.drop(columns=['index_new']) # The merge creates a new column \"index_new\", we don't care about that\n",
    "\n",
    "    # Some points don't lie inside any of the regions (this may be because of roundings in the coordinates)\n",
    "    # to these \"outside\" points, we're gonna attribute the nearest region to them\n",
    "    \n",
    "    # Datapoints which did not lie inside any region\n",
    "    left_out = df[df[f'{col}_new'].isnull()].copy()[['ID', 'geometry']]\n",
    "\n",
    "    # Merge on left_out points\n",
    "    left_out = left_out.sjoin_nearest(regions, how='inner', lsuffix='old', rsuffix='new')\n",
    "    left_out = left_out.drop(columns=['index_new', 'geometry'])\n",
    "\n",
    "    # Merge this new data onto original DF\n",
    "    df = df.merge(left_out, on=['ID'], how='left', suffixes=('', '2'))\n",
    "    \n",
    "    # Fill NULL values of new column with the buffered ones\n",
    "    df[f'{col}_new'] = df[f'{col}_new'].fillna(df[col])\n",
    "    df = df.drop(columns=[col])   # Drop this \"Buffered\" new column\n",
    "\n",
    "\n",
    "# Keep only wanted columns (just want ID and the respective new Region columns)\n",
    "# This is so when we merge to original dataset, only the new Region columns are created\n",
    "df = df[['ID'] + [f'{col}_new' for col in data_sources.keys()]]\n",
    "\n",
    "# Join the correct Ward/Community of each Beat to original dataframe\n",
    "chicago_crimes  = chicago_crimes.merge(df, how='left', on=['ID'])\n",
    "\n",
    "# Replacing \"normal\" columns with newest ones\n",
    "# Drop \"normal\" columns\n",
    "chicago_crimes = chicago_crimes.drop(columns = [col.replace('_new', '') for col in chicago_crimes.columns if '_new' in col])\n",
    "# Rename all the \"_new\" column to their respective name\n",
    "chicago_crimes = chicago_crimes.rename(columns = {col: col.replace('_new', '') for col in chicago_crimes.columns if '_new' in col})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing crimes which contain NaN in any feature\n",
    "\n",
    "Removing all rows which contain Null values except in Description features (we won't use description features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NA or empty values\n",
    "chicago_crimes_clean = chicago_crimes.dropna(subset=chicago_crimes.columns.difference(['Location Description', 'Description']), how='any', axis='index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Creating new features\n",
    "We create a set of new features that will help us identify patterns in the data, such as months with more crimes, periods of certain crimes, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date related features\n",
    "\n",
    "Features obtained in this section:\n",
    "* **Date**: Datetime truncated to day\n",
    "* **Hour**: Hour of the datetime\n",
    "* **YearMonth**: Datetime formatted to Year and Month concattenated\n",
    "* **DayOfWeek**: Week day of datetime (Monday/Wednesday/etc)\n",
    "* **periodOfDay**: If datetime occurred in morning/afternoon/etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map time to period of the day\n",
    "def map_time_to_period(hour):\n",
    "    if 5 <= hour < 12: return 'Morning'\n",
    "    elif 12 <= hour < 17: return 'Afternoon'\n",
    "    elif 17 <= hour < 21: return 'Evening'\n",
    "    else: return 'Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chicago_crimes_clean['Date_T'] = pd.to_datetime(chicago_crimes_clean['Date'].str[:16], format='%Y-%m-%dT%H:%M')\n",
    "chicago_crimes_clean['Date'] = pd.to_datetime(chicago_crimes_clean['Date'].str[:10], format='%Y-%m-%d')\n",
    "chicago_crimes_clean['Hour'] = chicago_crimes_clean.Date_T.dt.hour\n",
    "chicago_crimes_clean['YearMonth'] = chicago_crimes_clean['Date_T'].dt.strftime('%Y-%m')\n",
    "chicago_crimes_clean['Month'] = chicago_crimes_clean['Date_T'].dt.month\n",
    "chicago_crimes_clean['DayOfWeek'] = chicago_crimes_clean['Date_T'].dt.dayofweek\n",
    "del chicago_crimes_clean['Date_T']\n",
    "# Map the time column to periods of the day\n",
    "chicago_crimes_clean['periodOfDay'] = chicago_crimes_clean['Hour'].apply(map_time_to_period)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n",
    "\n",
    "Using the python script in the folder \"textProcessing\", we obtained a mapping between the descriptions and the new descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = pd.read_csv(r'./data/textProcessing/result/description_reduced.csv', header=None)\n",
    "desc.columns = ['Description_New', 'Description']\n",
    "loc_desc = pd.read_csv(r'./data/textProcessing/result/location_reduced.csv', header=None)\n",
    "loc_desc.columns = ['Location Description_New', 'Location Description']\n",
    "\n",
    "# Merge new descriptions onto original dataset\n",
    "chicago_crimes_clean = chicago_crimes_clean.merge(desc, on=['Description'], how='left')\n",
    "chicago_crimes_clean = chicago_crimes_clean.merge(loc_desc, on=['Location Description'], how='left')\n",
    "\n",
    "# Replace old columns with new columns\n",
    "for col in ['Location Description', 'Description']:\n",
    "    chicago_crimes_clean[col] = chicago_crimes_clean[f'{col}_New']\n",
    "    chicago_crimes_clean = chicago_crimes_clean.drop(columns=[f'{col}_New'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Datasets - Socio Economically Disadvantaged Areas\n",
    "\n",
    "Areas of Chicago, based on census tracts, that are the most socioeconomically disadvantaged, for the purpose of promoting equitable hiring within areas of economic need. Qualifying areas were identified using three criteria, based on data from the 2014 American Community Survey: household income, poverty rate, and unemployment rate.\n",
    "\n",
    "Socioeconomically Disadvantaged Areas -> https://data.cityofchicago.org/Community-Economic-Development/Socioeconomically-Disadvantaged-Areas/2ui7-wiq8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining dataset through the Endpoint API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a GeoDataFrame object from original dataset (to be able to perform geography operations)\n",
    "df = gpd.GeoDataFrame(chicago_crimes_clean, geometry=gpd.points_from_xy(chicago_crimes_clean['Longitude'], chicago_crimes_clean['Latitude']))[['ID', 'geometry']]\n",
    "\n",
    "\n",
    "# Url to get the data from (USING ENDPOINT API)\n",
    "url = 'https://data.cityofchicago.org/resource/2ui7-wiq8.json?$query=SELECT%20%60the_geom%60'\n",
    "\n",
    "# Fetching data from data_source (url obtained in DatasetDescription.docx document)\n",
    "raw_data = requests.get(url)\n",
    "raw_data = raw_data.json()  # Obtain the JSON response from data_source\n",
    "\n",
    "# Transform the geometry into a Shape object (easily read by GeoPandas)\n",
    "for (i, region) in enumerate(raw_data):\n",
    "    # Replace data with new data\n",
    "    raw_data[i] = {\n",
    "        # Create a \"Geometry\" column which will be the geometry read by GeoPandas\n",
    "        'geometry': geom.shape(region['the_geom'])\n",
    "    }\n",
    "\n",
    "# Converting the geometries to a GeoDataFrame (to be able to use geometry join)\n",
    "unadvantage_zones = gpd.GeoDataFrame(raw_data)\n",
    "# Creating new colmun to store the data (which regions are Unadvantaged)\n",
    "unadvantage_zones['Unadvantage Zone'] = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = folium.Map(location=[41.881832, -87.623177], zoom_start=11, tiles=\"CartoDB positron\")\n",
    "for _, r in unadvantage_zones.iterrows():\n",
    "    # Without simplifying the representation of each borough,\n",
    "    # the map might not be displayed\n",
    "    sim_geo = gpd.GeoSeries(r[\"geometry\"]).simplify(tolerance=0.001)\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j, style_function=lambda x: {\"fillColor\": \"orange\"})\n",
    "    geo_j.add_to(m)\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging regions into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data onto original (with geoDataFrame) dataset\n",
    "df = df.sjoin(unadvantage_zones, how='left', predicate='within', lsuffix='old', rsuffix='new') # Will merge regions onto DataFrame by checking in which region each datapoint is contained\n",
    "df = df.drop(columns=['index_new']) # The merge creates a new column \"index_new\", we don't care about that\n",
    "\n",
    "df['Unadvantage Zone'] = df['Unadvantage Zone'].fillna(False)\n",
    "\n",
    "\n",
    "# Join the data onto original dataset\n",
    "chicago_crimes_clean  = chicago_crimes_clean.merge(df[['ID', 'Unadvantage Zone']], how='left', on=['ID'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Data Visualization\n",
    "\n",
    "After preparing all the data, we're going to visualize all the dataset and everything that it has to offer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Crimes per month + Crimes by types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining figures and axes to plot data on\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "\n",
    "#count number of crimes per month\n",
    "chicago_crimes_monthly = chicago_crimes_clean.groupby(by=['YearMonth', 'Year', 'Month'], as_index=False)[['ID']].count()\n",
    "# Create a sequence of months\n",
    "month_labels = chicago_crimes_monthly['YearMonth'].unique().tolist()\n",
    "# Create the plot\n",
    "chicago_crimes_monthly.plot(\n",
    "    x='YearMonth'\n",
    "    ,y=['ID']\n",
    "    ,ax=axs[0]\n",
    "    ,title='Monthly Crimes')\n",
    "\n",
    "\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(chicago_crimes_monthly.set_index(['Year', 'Month'])['ID'].unstack().fillna(0), cmap='YlOrRd', linewidths=0.5, ax=axs[1])\n",
    "axs[1].set_xlabel('Month')\n",
    "axs[1].set_ylabel('Year')\n",
    "axs[1].set_title('Chicago Crimes Heatmap')\n",
    "\n",
    "\n",
    "# Crime By TYPES\n",
    "crime_by_type = chicago_crimes_clean['Primary Type'].value_counts().head(10)\n",
    "axs[2].set_title('Top 10 Crime Types')\n",
    "axs[2].set_xlabel('Crime Type')\n",
    "axs[2].set_ylabel('Number of Crimes')\n",
    "sns.barplot(x=crime_by_type.index, y=crime_by_type.values, ax=axs[2])\n",
    "axs[2].set_xticklabels(crime_by_type.index, rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Likelihood of an arrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = chicago_crimes[\"Arrest\"].value_counts()\n",
    "false = l[0]\n",
    "true = l[1]\n",
    "\n",
    "arrest = pd.DataFrame({'Status':['Not Arrested','Arrested'],'Value':list(l)})\n",
    "print(\"Percentage of no arrests of all reported crimes :\",false/(false+true)*100,'!')\n",
    "\n",
    "# How are arrests spread out across the months\n",
    "plt.style.use('bmh')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = sns.countplot(x=\"Year\",\n",
    "                   hue='Arrest',\n",
    "                   data=chicago_crimes_clean[['Year','Arrest']],\n",
    "                   palette=['Red', 'Green'])\n",
    "\n",
    "ax.set(title='Arrests Made per Year', xlabel='Year', ylabel='Number of Crimes')\n",
    "plt.title('Arrests Made per Year', fontdict={'fontsize': 20, 'color': 'black'}, weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Number of crimes per hour\n",
    "\n",
    "Aggregating number of crimes occurred in each hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plt.style.use('seaborn-dark')\n",
    "sns.set_context('paper')\n",
    "\n",
    "# Write code to plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.countplot(x='Hour', data=chicago_crimes_clean, palette=\"viridis\")\n",
    "\n",
    "# Aesthetic appeal\n",
    "plt.title(\"Unsafest Hours in Chicago\", fontdict={'fontsize': 15, 'color': '#bb0e14','fontname':'Agency FB'}, weight=\"bold\")\n",
    "plt.xlabel(\"\\nHour in the Day\", fontdict={'fontsize': 15}, weight='bold')\n",
    "plt.ylabel(\"Number of Crimes\\n\", fontdict={'fontsize': 15}, weight=\"bold\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Mapping yearly crimes per type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_crimes_sample = chicago_crimes_clean.sample(frac=0.1)\n",
    "# Create the map of Chicago crimes with primary type\n",
    "fig = px.scatter_mapbox(chicago_crimes_sample, animation_frame='Year',lat='Latitude', lon='Longitude',\n",
    "                        color='Primary Type', hover_name='Primary Type', hover_data=['Arrest','District','periodOfDay'],\n",
    "                        mapbox_style='open-street-map', height=800)\n",
    "\n",
    "# Customize the appearance and layout of the map\n",
    "fig.update_layout(\n",
    "    title='Chicago Crimes by Primary Type',\n",
    "    mapbox=dict(\n",
    "        zoom=10,\n",
    "        center=dict(lat=41.8781, lon=-87.6298)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the map\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Correlation between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(chicago_crimes_clean.corr(numeric_only=True),cmap='YlOrRd', linewidths=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Visualizing social unadvantaged regions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping crimes inside and outside regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_crimes_sample = chicago_crimes_clean.sample(frac=0.1).rename(columns={'Unadvantage Zone': 'Socioeconomically Disadvantaged Areas'})\n",
    "crimes_outside_zones = chicago_crimes_sample[~chicago_crimes_sample['Socioeconomically Disadvantaged Areas']]\n",
    "crimes_in_zones = chicago_crimes_sample[chicago_crimes_sample['Socioeconomically Disadvantaged Areas']]\n",
    "\n",
    "\n",
    "# Create the map of Chicago crimes with primary type\n",
    "fig = px.scatter_mapbox(chicago_crimes_sample, animation_frame='Year',lat='Latitude', lon='Longitude',\n",
    "                        color='Socioeconomically Disadvantaged Areas', color_discrete_sequence={False:'#8bb4f2', True:'#f28b8b'}, \n",
    "                        hover_name='Socioeconomically Disadvantaged Areas', hover_data=['Arrest','District'],\n",
    "                        mapbox_style='open-street-map', height=800)\n",
    "\n",
    "# Customize the appearance and layout of the map\n",
    "fig.update_layout(\n",
    "    mapbox=dict(\n",
    "        zoom=10,\n",
    "        center=dict(lat=41.8781, lon=-87.6298)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the map\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing crime types per region type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each crime type in non-socioeconomically disadvantaged areas\n",
    "crime_counts_outside = crimes_outside_zones['Primary Type'].value_counts().head(3)\n",
    "\n",
    "# Count occurrences of each crime type in socioeconomically disadvantaged areas\n",
    "crime_counts_in_zones = crimes_in_zones['Primary Type'].value_counts().head(3)\n",
    "\n",
    "# Create a single bar chart for both sets of data\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot for non-socioeconomically disadvantaged areas\n",
    "plt.bar(crime_counts_outside.index, crime_counts_outside.values, color='skyblue', alpha=0.8, label='Non-Socioeconomically Disadvantaged Areas')\n",
    "\n",
    "# Plot for socioeconomically disadvantaged areas\n",
    "plt.bar(crime_counts_in_zones.index, crime_counts_in_zones.values, color='lightcoral', alpha=0.8, label='Socioeconomically Disadvantaged Areas')\n",
    "\n",
    "plt.title('Comparison of Top Crime Types')\n",
    "plt.xlabel('Crime Type')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "\n",
    "# Add grid lines\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "# Adjust spacing between bars\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the combined plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Clustering with K-means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters to use for K-Means, and transforming them into numerical types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cols = ['Domestic', 'periodOfDay', 'Month', 'DayOfWeek', 'Beat', 'Primary Type', 'Arrest']\n",
    "df_aux = chicago_crimes_clean[kmeans_cols + ['Latitude', 'Longitude']].copy()\n",
    "\n",
    "for col in kmeans_cols:\n",
    "    dtype = df_aux[col].dtype\n",
    "    if str(dtype) in ['object', 'str', 'string']:\n",
    "        # Change type to category\n",
    "        df_aux[col] = df_aux[col].astype('category')\n",
    "        \n",
    "        # Convert from categorical to Numeric\n",
    "        df_aux[col] = df_aux[col].cat.codes\n",
    "    if str(dtype) == 'bool':\n",
    "        df_aux[col] = df_aux[col].astype('int8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Elbow Method\n",
    "\n",
    "In order to find the optimal number of clusters, we're going to use the elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_kwargs = {\n",
    "\"init\": \"random\",\n",
    "\"n_init\": 10,\n",
    "\"max_iter\": 300,\n",
    "\"random_state\": 42,\n",
    "}\n",
    "\n",
    "nrclusters_inertia = []\n",
    "for k in range(1,20):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(df_aux[kmeans_cols])\n",
    "\n",
    "    # inertia measures how well a dataset was clustered\n",
    "    nrclusters_inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 20), nrclusters_inertia)\n",
    "plt.xticks(range(1, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "\n",
    "kl = KneeLocator(range(1, 20), nrclusters_inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "print(\"The optimal number of clusters is -> \" + str(kl.elbow) + ' <- by the Elbow Method.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_kwargs = {\n",
    "\"init\": \"random\",\n",
    "\"n_init\": 10,\n",
    "\"max_iter\": 300,\n",
    "\"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Since 4 was  the optimal number of clusters, we are going to use 4.\n",
    "kmeans = KMeans(4, **kmeans_kwargs)\n",
    "\n",
    "# fitting the data\n",
    "kmeans.fit(df_aux[kmeans_cols])\n",
    "\n",
    "# Clusters of each datapoint\n",
    "df_aux['Cluster'] = kmeans.predict(df_aux[kmeans_cols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Clusters per beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = df_aux.sample(frac=0.05)\n",
    "# Create the map of Chicago crimes with primary type\n",
    "fig = px.scatter_mapbox(df_sample, lat='Latitude', lon='Longitude',\n",
    "                        color='Cluster',\n",
    "                        mapbox_style='open-street-map', height=800)\n",
    "\n",
    "# Customize the appearance and layout of the map\n",
    "fig.update_layout(\n",
    "    title='K-Means clustering',\n",
    "    mapbox=dict(\n",
    "        zoom=10,\n",
    "        center=dict(lat=41.8781, lon=-87.6298)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the map\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Association Rules\n",
    "\n",
    "In this section we perform the association rules from the dataset with the objective to learn eventual co-occurrence of crimes.\n",
    "\n",
    "For this we use first the Apriori method and later the FP-Growth.\n",
    "\n",
    "Its valid to note that we used a sample from the original dataset due to the fact that it was to large to process in reasonable time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Item Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = chicago_crimes_clean.sample(frac=0.04)\n",
    "\n",
    "# Preprocess the dataset and select relevant columns\n",
    "df = df_sample[['Primary Type', 'Location Description', 'Date', 'Beat']]\n",
    "\n",
    "# Convert the dataset into a transactional format\n",
    "transactions = df.groupby(['Date'])['Primary Type'].apply(list).values.tolist()\n",
    "\n",
    "# Encode the transactional data\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit_transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "\n",
    "# Calculate item frequency (support)\n",
    "item_support = df_encoded.sum() / len(df_encoded)\n",
    "\n",
    "# Sort the items by support in descending order\n",
    "sorted_items = item_support.sort_values(ascending=False)\n",
    "\n",
    "# Plot the item frequency\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sorted_items.index, sorted_items.values)\n",
    "plt.xlabel('Item')\n",
    "plt.ylabel('Support')\n",
    "plt.title('Item Frequency (Support)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "df_association_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "frequent_itemsets = frequent_itemsets.sort_values('support', ascending=False)\n",
    "\n",
    "# Filter rules based on desired metrics or thresholds\n",
    "filtered_rules = df_association_rules[(df_association_rules['confidence'] > 0.7) & (df_association_rules['lift'] > 1.2)]\n",
    "filtered_rules = filtered_rules.sort_values(by = 'support', ascending = False)\n",
    "\n",
    "# Print the association rules\n",
    "for index, rule in filtered_rules.iterrows():\n",
    "    antecedents = ', '.join(list(rule['antecedents']))\n",
    "    consequents = ', '.join(list(rule['consequents']))\n",
    "    print(f\"Rule: {antecedents} -> {consequents}\")\n",
    "    print(f\"Support: {rule['support']}\")\n",
    "    print(f\"Confidence: {rule['confidence']}\")\n",
    "    print(f\"Lift: {rule['lift']}\")\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the association rules\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatterplot = sns.scatterplot(data=filtered_rules, x='support', y='confidence', size='lift', hue='lift', palette='viridis')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Association Rules')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP-Growth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "df = df_sample[['Primary Type', 'Location Description', 'Beat', 'Date']].copy()\n",
    "\n",
    "# Convert the dataset into a transactional format\n",
    "transactions = df.groupby(['Date'])['Primary Type'].apply(list).values.tolist()\n",
    "\n",
    "# Encode the transactional data\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit_transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "\n",
    "# Apply FP-Growth algorithm to find frequent itemsets\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "df_association_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rules based on desired metrics or thresholds\n",
    "filtered_rules = df_association_rules[(df_association_rules['confidence'] > 0.8) & (df_association_rules['lift'] > 1.2)]\n",
    "\n",
    "# Print the association rules\n",
    "for index, rule in filtered_rules.iterrows():\n",
    "    antecedents = ', '.join(list(rule['antecedents']))\n",
    "    consequents = ', '.join(list(rule['consequents']))\n",
    "    print(f\"Rule: {antecedents} -> {consequents}\")\n",
    "    print(f\"Support: {rule['support']}\")\n",
    "    print(f\"Confidence: {rule['confidence']}\")\n",
    "    print(f\"Lift: {rule['lift']}\")\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "# Sort the association rules by lift\n",
    "association_rules_graph = filtered_rules.sort_values(by='lift', ascending=False)\n",
    "\n",
    "# Plot the association rules by lift\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='lift', y='antecedents', data=association_rules_graph.head(10))\n",
    "plt.xlabel('Lift')\n",
    "plt.ylabel('Antecedents')\n",
    "plt.title('Top 10 Association Rules by Lift')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the association rules\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatterplot = sns.scatterplot(data=filtered_rules, x='support', y='confidence', size='lift', hue='lift', palette='viridis')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Association Rules')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Link Analysis\n",
    "\n",
    "## CrimeType -> Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for link analysis\n",
    "df = df_sample[['Primary Type', 'Location Description', 'Arrest', 'Domestic', 'Beat', 'Year', 'District']].head(100).copy()\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Iterate over the dataset and add edges to the graph\n",
    "for _, row in df.iterrows():\n",
    "    crime_type = row['Primary Type']\n",
    "    location = row['Location Description']\n",
    "    G.add_edge(crime_type, location)\n",
    "\n",
    "# Perform link analysis using PageRank algorithm\n",
    "page_rank = nx.pagerank(G)\n",
    "\n",
    "# Sort the nodes based on PageRank score\n",
    "sorted_nodes = sorted(page_rank, key=page_rank.get, reverse=True)\n",
    "\n",
    "# Print the top 10 nodes by PageRank score\n",
    "for node in sorted_nodes[:10]:\n",
    "    print(f\"Node: {node}, PageRank Score: {page_rank[node]}\")\n",
    "\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G, k=0.3)\n",
    "nx.draw_networkx(G, pos, with_labels=True, node_size=1500, node_color='lightblue', edge_color='gray', font_size=10, width=1, alpha=0.7)\n",
    "plt.title('Crime-related Entities Graph')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using some other features\n",
    "\n",
    "The features used in this link analysis were:\n",
    "* Primary Type\n",
    "* Description\n",
    "* Location\n",
    "* Arrest\n",
    "* Domestic\n",
    "* Year\n",
    "* Month\n",
    "* Weekday\n",
    "* WeekMonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add folder to path (to run code correctly)\n",
    "sys.path.insert(0, './data/linkAnalysis')\n",
    "import reportFinal\n",
    "\n",
    "# Remove linkanalysis folder from sys path\n",
    "sys.path.pop(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Predictive Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining predicted feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_attr = 'Arrest'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'Location Description'\n",
    "    ,'Description'\n",
    "    ,'Unadvantage Zone'\n",
    "    ,'DayOfWeek'\n",
    "    ,'Month'\n",
    "    ,'periodOfDay'\n",
    "    ,'Beat'\n",
    "    ,'Primary Type'\n",
    "    ,'Domestic'\n",
    "]\n",
    "\n",
    "df_final = chicago_crimes_clean[features + [prediction_attr]].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting all features to numerical Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergint all columns to numerical type\n",
    "for col in df_final.columns:\n",
    "    dtype = df_final[col].dtype\n",
    "    if str(dtype) in ['object', 'str', 'string']:\n",
    "        # Change type to category\n",
    "        df_final[col] = df_final[col].astype('category')\n",
    "\n",
    "        # Convert from categorical to Numeric\n",
    "        df_final[col] = df_final[col].cat.codes\n",
    "    if str(dtype) == 'bool':\n",
    "        df_final[col] = df_final[col].astype('int8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the logistic regression model\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Search\n",
    "\n",
    "In this section we're are looking for the best parameters to include in our XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter space variables\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 35, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 0,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.1,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'learning_rate': hp.quniform('learning_rate', 0, 1, 0.05),\n",
    "        'n_estimators': 30,\n",
    "        'seed': 123\n",
    "    }\n",
    "\n",
    "\n",
    "# Defining training and testing sets\n",
    "train = df_final.loc[[x for x in df_final.index if np.random.random()<0.7]].copy()\n",
    "test  = df_final.loc[~df_final.index.isin(train.index)].copy()\n",
    "\n",
    "\n",
    "# search function for hyperparameter\n",
    "def objective(space):\n",
    "    clf=xgb.XGBClassifier(objective ='reg:logistic', booster='gbtree', learning_rate = space['learning_rate'],\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree']), eval_metric=\"auc\", early_stopping_rounds=10)\n",
    "    \n",
    "    evaluation = [(train[features], train[prediction_attr]), (test[features], test[prediction_attr])]\n",
    "    \n",
    "    clf.fit(train[features], train[prediction_attr],\n",
    "            eval_set=evaluation, verbose=0)\n",
    "    \n",
    "\n",
    "    pred = clf.predict(test[features])\n",
    "    accuracy = accuracy_score(test[prediction_attr], pred>0.5)\n",
    "    # print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "# run hyperparameter function\n",
    "trials = Trials()\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)\n",
    "\n",
    "# show results\n",
    "print(\"The best hyperparameters are : \")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Model\n",
    "\n",
    "In this section, we're going to make use of the `xgboost` python package which contains a `XGBClassifier` class which allows for the modeling of a *XGBoost* model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoostModel = xgb.XGBClassifier(\n",
    "    objective ='reg:logistic'\n",
    "    ,colsample_bytree = 0.6038\n",
    "    ,learning_rate = 0.8\n",
    "    ,max_depth = 11\n",
    "    ,n_estimators = 30\n",
    "    ,min_split_loss=5\n",
    "    ,booster='gbtree'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Search\n",
    "\n",
    "After obtaining the features to use in the model, we perform a hyperparameter search to try to find the best arguments to use on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['auto', 'sqrt'],  # Number of features to consider when looking for the best split\n",
    "    'random_state': [42]  # Random state for reproducibility\n",
    "}\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Defining new train and test dataset with new selected features\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final[features], df_final['Arrest'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Perform randomized search with cross-validation\n",
    "random_search = RandomizedSearchCV(rf_model, param_distributions=param_grid, n_iter=5, cv=3, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Make predictions on the testing set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the best hyperparameters and accuracy score\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Model\n",
    "\n",
    "We then input all the discovered hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "n_estimators = 100\n",
    "min_samples_split = 10\n",
    "min_samples_leaf = 4\n",
    "max_features = 'sqrt'\n",
    "max_depth = None\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=random_state,\n",
    "                                       n_estimators=n_estimators,\n",
    "                                       min_samples_split=min_samples_split,\n",
    "                                       min_samples_leaf=min_samples_leaf,\n",
    "                                       max_features=max_features,\n",
    "                                       max_depth=max_depth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBMModel = lgb.LGBMClassifier(\n",
    "    n_estimators=400, random_state=42,\n",
    "    num_leaves= 20, max_depth=5, \n",
    "    silent=True, \n",
    "    metric='None', \n",
    "    n_jobs=6, \n",
    "    colsample_bytree=0.9,\n",
    "    subsample=0.9,\n",
    "    learning_rate=0.3\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Class\n",
    "In order to compare all our different models, we decided to create a class which, given a certan model and dataset, will evaluate the given method with some different methods. Afterwards we'll apply this class to all the model defined previously and compare them between each other\n",
    "\n",
    "The methods included:\n",
    "* Holdout Method\n",
    "* Random SubSampling\n",
    "* Startified K Folding\n",
    "* Leave One Out Cross Validation\n",
    "* ROC + AUC score\n",
    "* Confusion Matrix\n",
    "\n",
    "**NOTE**: Despiting `Leave_One_Out_Cross_Validation` being implemented, it might not be ran since it is very complex to run and it took +/- 8 hours to run. We decided to exclude it in the method `run_all_evaluations` by default for its complexity and for not affecting the results by our tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalModel:\n",
    "    def __init__(self, model=None, model_name='', dataset: pd.DataFrame=None, prediction_attrs=None):\n",
    "        self.model = model\n",
    "        self.label = model_name\n",
    "        self.df = dataset.copy()\n",
    "\n",
    "        if not isinstance(prediction_attrs, list): prediction_attrs=[prediction_attrs]\n",
    "        self.pred_attrs=prediction_attrs\n",
    "\n",
    "    # Evaluation metrics\n",
    "    def _mean_prediction_error(self, real, pred):\n",
    "        return np.average(pred-real)\n",
    "\n",
    "    def _standard_error(self, real, pred):\n",
    "        return np.std(pred-real)\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    def evaluate(self, print_evaluation=True):\n",
    "        test = self.test if not isinstance(self.train, list) else self.test[-1]\n",
    "\n",
    "        self.real = test[self.pred_attrs]\n",
    "        self.pred = self.model.predict(test.drop(columns=self.pred_attrs)) \n",
    "        \n",
    "        if len(self.pred_attrs) == 1:\n",
    "            self.real = self.real.to_numpy().T[0]\n",
    "\n",
    "        self.std_err = self._standard_error(self.real, self.pred)\n",
    "        self.avg_err = self._mean_prediction_error(self.real, self.pred)\n",
    "        \n",
    "        \n",
    "        if print_evaluation: print(f'<{self.label}> {self.method} Error: {self.avg_err:.3f} +/- {self.std_err:.3f}')\n",
    "        return self.avg_err\n",
    "\n",
    "    # Fitting model\n",
    "    def fit(self):\n",
    "        if isinstance(self.train, list):\n",
    "            return self.model.fit(self.train[-1].drop(columns=self.pred_attrs), self.train[-1][self.pred_attrs])\n",
    "        return self.model.fit(self.train.drop(columns=self.pred_attrs), self.train[self.pred_attrs])\n",
    "\n",
    "\n",
    "    # Defining evaluation methods\n",
    "    def Houldout_Method(self, train_ratio=0.7, print_evaluation=True, **kwargs):\n",
    "        self.method = 'Holdout Method'\n",
    "        self.train  = self.df.iloc[:int(self.df.shape[0]*train_ratio)].copy()\n",
    "        self.test   = self.df.loc[~self.df.index.isin(self.train.index)].copy()\n",
    "\n",
    "        self.fit()\n",
    "        self.evaluate(print_evaluation=print_evaluation)\n",
    "\n",
    "        \n",
    "    def Random_Subsampling(self, seed=1, val_ratio=0.7, print_evaluation=True):\n",
    "        np.random.seed=seed\n",
    "        self.method = 'Random SubSampling'\n",
    "\n",
    "        self.train = self.df.loc[[x for x in self.df.index if np.random.random()<val_ratio]].copy()\n",
    "        self.test  = self.df.loc[~self.df.index.isin(self.train.index)].copy()\n",
    "\n",
    "        self.fit()\n",
    "        self.evaluate(print_evaluation=print_evaluation)\n",
    "        \n",
    "\n",
    "\n",
    "    def Stratified_K_Fold(self, seed=1, n_splits=10, print_folds=True, print_evaluation=True):\n",
    "        np.random.seed=seed\n",
    "        self.method='k-fold Cross Validation'\n",
    "\n",
    "        # pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth))\n",
    "        strtfdKFold = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "        kfold = strtfdKFold.split(self.df.drop(columns=self.pred_attrs), self.df[self.pred_attrs])\n",
    "        scores = []\n",
    "\n",
    "        self.train = []\n",
    "        self.test = []\n",
    "        for k, (train, test) in enumerate(kfold):\n",
    "            self.train.append(self.df.iloc[train])\n",
    "            self.test.append(self.df.iloc[test])\n",
    "\n",
    "            self.fit()\n",
    "            \n",
    "            score = self.evaluate(print_evaluation=False)\n",
    "            scores.append(score)\n",
    "            if print_folds: print(f'Fold: {k+1:2d}, Training/Test Split Distribution: {np.bincount(self.df[self.pred_attrs].iloc[test].to_numpy().T[0])}, Error: {score:.3f}')\n",
    "\n",
    "        self.avg_err = np.mean(scores)\n",
    "        self.std_err = np.std(scores)\n",
    "        if print_evaluation: print(f'<{self.label}> {self.method} Error: {self.avg_err:.3f} +/- {self.std_err:.3f}')\n",
    "        \n",
    "    def Leave_One_Out_Cross_Validation(self, print_folds=True, print_evaluation=True, **kwargs):\n",
    "        self.method='Leave One Out Cross Validation'\n",
    "        \n",
    "        leaveOneOut = LeaveOneOut()\n",
    "        kfold = leaveOneOut.split(self.df.drop(columns=self.pred_attrs), self.df[self.pred_attrs])\n",
    "        scores = []\n",
    "        \n",
    "        self.train = []\n",
    "        self.test = []\n",
    "        for k, (train, test) in enumerate(kfold):\n",
    "            self.train.append(self.df.iloc[train])\n",
    "            self.test.append(self.df.iloc[test])\n",
    "\n",
    "            self.fit()\n",
    "            \n",
    "            score = self.evaluate(print_evaluation=False)\n",
    "            scores.append(score)\n",
    "            if print_folds: print(f'Fold: {k+1:2d}, Training/Test Split Distribution: {np.bincount(self.df[self.pred_attrs].iloc[test].to_numpy().T[0])}, Error: {score:.3f}')\n",
    "\n",
    "        self.avg_err = np.mean(scores)\n",
    "        self.std_err = np.std(scores)\n",
    "        if print_evaluation: print(f'<{self.label}> {self.method} Error: {self.avg_err:.3f} +/- {self.std_err:.3f}')\n",
    "\n",
    "    def Bootstrap_Method(self, n_tries=200, print_evaluation=True, **kwargs):\n",
    "        scores = []\n",
    "        for _ in range(n_tries):\n",
    "            self.Random_Subsampling(print_evaluation=False, **kwargs)\n",
    "            \n",
    "            \n",
    "            scores.append(self.evaluate(print_evaluation=False))\n",
    "        self.evaluate(print_evaluation=print_evaluation)\n",
    "        self.method = 'Bootstrap Method'\n",
    "    \n",
    "    def ROC(self, figsize=(8, 6), metric_index_to_compare=0, print_auc_score=True):\n",
    "        \n",
    "        self.Random_Subsampling(print_evaluation=False)\n",
    "\n",
    "        self.real = self.test[self.pred_attrs[metric_index_to_compare]].to_numpy().T\n",
    "        self.pred = self.model.predict(self.test.drop(columns=self.pred_attrs))\n",
    "        # ROC Curve\n",
    "        self.roc_curve = roc_curve(self.real, self.pred)\n",
    "        self.false_positive_rate = self.roc_curve[0]\n",
    "        self.true_positive_rate = self.roc_curve[1]\n",
    "        self.threshholds = self.roc_curve[2]\n",
    "        \n",
    "        self.roc_auc_score = roc_auc_score(self.real, self.pred)\n",
    "        if print_auc_score: print(f'<{self.label}> ROC AUC score: {self.roc_auc_score:.3f}')\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        ax = plt.gca()\n",
    "        ax.plot(self.false_positive_rate, self.true_positive_rate)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve <{self.label}>')\n",
    "        plt.show()\n",
    "\n",
    "    def conf_matrix(self, metric_index_to_compare=0, figsize=(6, 6)):\n",
    "        class_names = np.array([0, 1])\n",
    "\n",
    "        # Train and divide into test and train\n",
    "        self.Random_Subsampling(print_evaluation=False)\n",
    "\n",
    "        # Real and Predicted values of test set\n",
    "        self.real = self.test[self.pred_attrs[metric_index_to_compare]].to_numpy().T\n",
    "        self.pred = np.vectorize(lambda x: min(x, 1))(self.model.predict(self.test.drop(columns=self.pred_attrs)).round())\n",
    "\n",
    "        matrix = confusion_matrix(self.real, self.pred)\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.set(font_scale=1)\n",
    "        sns.heatmap(matrix, annot=True, annot_kws={'size':10}, cmap=plt.cm.Greens, linewidths=0.2, fmt='g')\n",
    "\n",
    "        # Add labels to the plot\n",
    "        tick_marks = class_names\n",
    "        tick_marks2 = tick_marks + 0.5\n",
    "        plt.xticks(tick_marks + 0.5, class_names, rotation=0)\n",
    "        plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.ylabel('True label')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def run_all_evaluations(self, leave_one_out=False, stratified_splits=10):\n",
    "        self.Houldout_Method()\n",
    "        self.Random_Subsampling()\n",
    "        self.Stratified_K_Fold(print_folds=False, n_splits=stratified_splits)\n",
    "        if leave_one_out: self.Leave_One_Out_Cross_Validation(print_folds=False)\n",
    "        self.ROC()\n",
    "        self.conf_matrix()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"#\":#^50s}\\n#{\"Evaluating <Logistic Regression>\":^48s}#\\n{\"#\":#^50s}')\n",
    "model = 'Logistic Regression'\n",
    "logreg_eval = EvalModel(logreg, model_name=model, dataset=df_final, prediction_attrs=prediction_attr)\n",
    "logreg_eval.run_all_evaluations()\n",
    "\n",
    "print(f'{\"#\":#^50s}\\n#{\"Evaluating <XGBoost>\":^48s}#\\n{\"#\":#^50s}')\n",
    "model = 'XGBoost'\n",
    "xgboost_eval = EvalModel(XGBoostModel, model_name=model, dataset=df_final, prediction_attrs=prediction_attr)\n",
    "xgboost_eval.run_all_evaluations()\n",
    "\n",
    "print(f'{\"#\":#^50s}\\n#{\"Evaluating <Random Forest>\":^48s}#\\n{\"#\":#^50s}')\n",
    "model = 'Random Forest'\n",
    "rf_eval = EvalModel(rf_classifier, model_name=model, dataset=df_final, prediction_attrs=prediction_attr)\n",
    "rf_eval.run_all_evaluations()\n",
    "\n",
    "print(f'{\"#\":#^50s}\\n#{\"Evaluating <LGBM>\":^48s}#\\n{\"#\":#^50s}')\n",
    "model = 'LGBM'\n",
    "rf_eval = EvalModel(LGBMModel, model_name=model, dataset=df_final, prediction_attrs=prediction_attr)\n",
    "rf_eval.run_all_evaluations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
